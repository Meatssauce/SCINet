{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-09-05T19:15:11.520039Z",
     "end_time": "2023-09-05T19:15:13.954450Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "from preprocessing import TimeSeriesPreprocessor\n",
    "from SCINet import SCINet, StackedSCINet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def make_stackedSciNet(input_shape, output_shape):\n",
    "    inputs = tf.keras.Input(shape=(input_shape[1], input_shape[2]), name='inputs')\n",
    "    x = StackedSCINet(horizon, features=input_shape[-1], stacks=K, levels=L, h=h, kernel_size=kernel_size)(inputs)\n",
    "    model = tf.keras.Model(inputs, x)\n",
    "\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model, to_file='modelDiagram.png', show_shapes=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_sciNet(input_shape, output_shape):\n",
    "    inputs = tf.keras.Input(shape=(input_shape[1], input_shape[2]), name='inputs')\n",
    "    x = SCINet(horizon, features=input_shape[-1], levels=L, h=h, kernel_size=kernel_size)(inputs)\n",
    "    model = tf.keras.Model(inputs, x)\n",
    "\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model, to_file='modelDiagram.png', show_shapes=True)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-05T19:15:13.956152Z",
     "end_time": "2023-09-05T19:15:13.957977Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data_filepath = 'datasets/ETDataset-main/ETT-small/ETTh1.csv'\n",
    "y_col = 'OT'\n",
    "index_col = 'date'\n",
    "\n",
    "# Hyperparams\n",
    "degree_of_differencing = 0\n",
    "# T, tilta\n",
    "look_back_window, horizon = 48, 24\n",
    "batch_size = 16\n",
    "learning_rate = 3e-3\n",
    "h, kernel_size, L, K = 4, 5, 3, 2\n",
    "l1, l2 = 0, 0\n",
    "# split_strides = look_back_window + horizon\n",
    "split_strides = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-05T19:15:13.958642Z",
     "end_time": "2023-09-05T19:15:13.960458Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv(data_filepath, index_col=index_col).astype('float32')\n",
    "\n",
    "train_data = data[:int(0.6 * len(data))]\n",
    "val_data = data[int(0.6 * len(data)):int(0.8 * len(data))]\n",
    "test_data = data[int(0.8 * len(data)):]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-05T19:15:13.961595Z",
     "end_time": "2023-09-05T19:15:13.981337Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: X(10381, 48, 7), y(10381, 24, 7)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 48, 7)]           0         \n",
      "                                                                 \n",
      " stacked_sci_net (StackedSC  (None, 24, 7)             224952    \n",
      " INet)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 224952 (878.72 KB)\n",
      "Trainable params: 224952 (878.72 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "preprocessor = TimeSeriesPreprocessor(look_back_window, horizon, split_strides, degree_of_differencing,\n",
    "                                      relative_diff=False, scaling='standard')\n",
    "X_train, y_train = preprocessor.fit_transform(train_data)\n",
    "X_val, y_val = preprocessor.transform(val_data)\n",
    "print(f'Input shape: X{X_train.shape}, y{y_train.shape}')\n",
    "\n",
    "model = make_stackedSciNet(X_train.shape, y_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-05T19:15:13.981749Z",
     "end_time": "2023-09-05T19:15:14.753275Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.6425275802612305\n",
      "Validation MAE: 0.4695684313774109\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.40406882762908936\n",
      "Validation MAE: 0.4374469220638275\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.3823499083518982\n",
      "Validation MAE: 0.42486223578453064\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.37281474471092224\n",
      "Validation MAE: 0.4271564781665802\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.36914053559303284\n",
      "Validation MAE: 0.4143417775630951\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.36939698457717896\n",
      "Validation MAE: 0.4305734634399414\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.36861634254455566\n",
      "Validation MAE: 0.43555325269699097\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.37048736214637756\n",
      "Validation MAE: 0.4153284430503845\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.3729897141456604\n",
      "Validation MAE: 0.42487865686416626\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.3719906210899353\n",
      "Validation MAE: 0.4315313696861267\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 150\n",
    "\n",
    "# Initialize early stopping parameters\n",
    "best_val_mae = float('inf')\n",
    "patience = 10  # for early stopping\n",
    "counter = 0\n",
    "\n",
    "# Create optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# optimizer.build(model.trainable_weights)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Split data into batches\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=1024, seed=4321).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.shuffle(buffer_size=1024, seed=4321).batch(batch_size)\n",
    "\n",
    "# MAE metrics\n",
    "train_mae_metric = tf.keras.metrics.MeanAbsoluteError()\n",
    "val_mae_metric = tf.keras.metrics.MeanAbsoluteError()\n",
    "\n",
    "\n",
    "# Custom training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # Initialize tqdm with the number of steps (batches) per epoch\n",
    "    prog_bar = tqdm(dataset, unit=\"step\", leave=False)\n",
    "\n",
    "    for step, (x_batch, y_batch) in enumerate(prog_bar):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x_batch)\n",
    "\n",
    "            # Forward pass\n",
    "            y = model(x_batch, training=True)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss1 = loss_fn(y_batch, model.layers[1].outputs[0])\n",
    "            loss2 = loss_fn(y_batch, model.layers[1].outputs[1])\n",
    "            total_loss = loss1 + loss2\n",
    "\n",
    "            # Update tqdm description with current loss\n",
    "            prog_bar.set_description(f\"Training loss: {total_loss:.4f}\")\n",
    "\n",
    "        # Update MAE metric for training data\n",
    "        train_mae_metric.update_state(y_batch, y)\n",
    "\n",
    "        # Calculate gradients\n",
    "        grads1 = tape.gradient(loss1, model.layers[1].scinets[0].trainable_weights)\n",
    "        grads2 = tape.gradient(loss2, model.layers[1].scinets[1].trainable_weights)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.apply_gradients(zip(grads1, model.layers[1].scinets[0].trainable_weights))\n",
    "        optimizer.apply_gradients(zip(grads2, model.layers[1].scinets[1].trainable_weights))\n",
    "\n",
    "        # Clean up resources of the tape\n",
    "        del tape\n",
    "\n",
    "    # Validation loop\n",
    "    for x_val_batch, y_val_batch in val_dataset:\n",
    "        val_predictions = model(x_val_batch)\n",
    "        val_mae_metric.update_state(y_val_batch, val_predictions)\n",
    "\n",
    "    # Show metrics\n",
    "    print(f\"Training MAE: {train_mae_metric.result()}\")\n",
    "    print(f\"Validation MAE: {val_mae_metric.result()}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_mae_metric.result() < best_val_mae:\n",
    "        best_val_mae = val_mae_metric.result()\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    # Reset metrics for the next epoch\n",
    "    train_mae_metric.reset_states()\n",
    "    val_mae_metric.reset_states()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-05T19:15:14.753607Z",
     "end_time": "2023-09-05T19:36:19.906894Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Generate new id and create save directory\n",
    "existing_ids = [int(name) for name in os.listdir('saved-models/') if name.isnumeric()]\n",
    "run_id = random.choice(list(set(range(0, 1000)) - set(existing_ids)))\n",
    "save_directory = f'saved-models/regressor/{run_id:03d}/'\n",
    "os.makedirs(os.path.dirname(save_directory), exist_ok=True)\n",
    "\n",
    "# Save model, preprocessor and training history\n",
    "model.save(save_directory)\n",
    "with open(save_directory + 'preprocessor', 'wb') as f:\n",
    "    dump(preprocessor, f, compress=3)\n",
    "pd.DataFrame(history.history).to_csv(save_directory + 'train_history.csv')\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('mean absolute error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.savefig(save_directory + 'accuracy.png')\n",
    "plt.clf()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.savefig(save_directory + 'loss.png')\n",
    "\n",
    "# Evaluate\n",
    "# run_id = 186\n",
    "# model = load_model(f'saved-models/regressor/{run_id:03d}/')\n",
    "# with open(f'saved-models/regressor/{run_id:03d}/preprocessor', 'rb') as f:\n",
    "#     preprocessor = load(f)\n",
    "X_test, y_test = preprocessor.transform(test_data)\n",
    "scores = model.evaluate({'inputs': X_test, 'targets': y_test})\n",
    "\n",
    "# Save evaluation results\n",
    "if not isinstance(scores, list):\n",
    "    scores = [scores]\n",
    "row = [run_id] + scores + [pd.Timestamp.now(tz='Australia/Melbourne')]\n",
    "try:\n",
    "    df_scores = pd.read_csv('saved-models/scores.csv')\n",
    "    df_scores.loc[len(df_scores)] = row\n",
    "except (FileNotFoundError, ValueError):\n",
    "    df_scores = pd.DataFrame([row], columns=['id'] + list(model.metrics_names) + ['time'])\n",
    "df_scores.to_csv('saved-models/scores.csv', index=False)\n",
    "\n",
    "# # Predict\n",
    "# # y_test is only used to calculate loss, how to get rid of it?\n",
    "# y_pred = model.predict({'inputs': X_test, 'targets': y_test})\n",
    "# y_pred = preprocessor.scaler.inverse_transform(y_pred.reshape(-1, y_test.shape[-1]))\n",
    "# y_test = preprocessor.scaler.inverse_transform(y_test.reshape(-1, y_test.shape[-1]))\n",
    "# comparison = np.hstack([y_pred, y_test])\n",
    "# df = pd.DataFrame(comparison, columns=['Predicted', 'Actual'])\n",
    "# df.to_csv(f'saved-models/regressor/{run_id:03d}/comparison.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
