{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-09-05T21:40:37.197011Z",
     "end_time": "2023-09-05T21:40:40.037498Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from preprocessing import TimeSeriesPreprocessor\n",
    "from SCINet import SCINet, StackedSCINet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def make_stackedSciNet(input_shape):\n",
    "    inputs = tf.keras.Input(shape=(input_shape[1], input_shape[2]), name='inputs')\n",
    "    x = StackedSCINet(horizon, features=input_shape[-1], stacks=K, levels=L, h=h,\n",
    "                      kernel_size=kernel_size, dropout=dropout)(inputs)\n",
    "    model = tf.keras.Model(inputs, x)\n",
    "\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model, to_file='modelDiagram.png', show_shapes=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_sciNet(input_shape):\n",
    "    inputs = tf.keras.Input(shape=(input_shape[1], input_shape[2]), name='inputs')\n",
    "    x = SCINet(horizon, features=input_shape[-1], levels=L, h=h,\n",
    "               kernel_size=kernel_size, dropout=dropout)(inputs)\n",
    "    model = tf.keras.Model(inputs, x)\n",
    "\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model, to_file='modelDiagram.png', show_shapes=True)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-05T21:40:40.039727Z",
     "end_time": "2023-09-05T21:40:40.040928Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data_filepath = 'datasets/ETDataset-main/ETT-small/ETTh1.csv'\n",
    "y_col = 'OT'\n",
    "index_col = 'date'\n",
    "\n",
    "# Hyperparams\n",
    "degree_of_differencing = 0\n",
    "# T, tilta\n",
    "look_back_window, horizon = 96, 48\n",
    "batch_size = 16\n",
    "learning_rate = 9e-3\n",
    "h, kernel_size, L, K = 4, 5, 3, 1\n",
    "l1, l2 = 0, 0\n",
    "dropout = 0.25\n",
    "# split_strides = look_back_window + horizon\n",
    "split_strides = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-05T21:40:40.041862Z",
     "end_time": "2023-09-05T21:40:40.043194Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv(data_filepath, index_col=index_col).astype('float32')\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "train_data = data[:int(0.6 * len(data))]\n",
    "val_data = data[int(0.6 * len(data)):int(0.8 * len(data))]\n",
    "test_data = data[int(0.8 * len(data)):]\n",
    "\n",
    "# # Get the minimum and maximum dates in the DataFrame\n",
    "# min_date = data.index.min()\n",
    "# max_date = data.index.max()\n",
    "#\n",
    "# # Calculate the splitting points\n",
    "# train_end_date = min_date + pd.DateOffset(months=12) - pd.DateOffset(days=1)\n",
    "# val_end_date = train_end_date + pd.DateOffset(months=4)\n",
    "# test_end_date = val_end_date + pd.DateOffset(months=4)\n",
    "#\n",
    "# # Split into train, val, and test sets based on the calculated dates\n",
    "# train_data = data[data.index <= train_end_date]\n",
    "# val_data = data[(data.index > train_end_date) & (data.index <= val_end_date)]\n",
    "# test_data = data[(data.index > val_end_date) & (data.index <= test_end_date)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-05T21:40:40.045227Z",
     "end_time": "2023-09-05T21:40:40.067489Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: X(10309, 96, 7), y(10309, 48, 7)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 96, 7)]           0         \n",
      "                                                                 \n",
      " stacked_sci_net (StackedSC  (None, 48, 7)             281988    \n",
      " INet)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 281988 (1.08 MB)\n",
      "Trainable params: 281988 (1.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "preprocessor = TimeSeriesPreprocessor(look_back_window, horizon, split_strides, degree_of_differencing,\n",
    "                                      relative_diff=False, scaling='standard')\n",
    "X_train, y_train = preprocessor.fit_transform(train_data)\n",
    "X_val, y_val = preprocessor.transform(val_data)\n",
    "print(f'Input shape: X{X_train.shape}, y{y_train.shape}')\n",
    "\n",
    "model = make_stackedSciNet(X_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-05T21:40:40.068857Z",
     "end_time": "2023-09-05T21:40:40.579277Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = f'logs/gradient_tape/{current_time}/train'\n",
    "val_log_dir = f'logs/gradient_tape/{current_time}/val'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-05T21:40:40.581737Z",
     "end_time": "2023-09-05T21:40:40.584513Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.5570996403694153\n",
      "Validation MAE: 0.6282496452331543\n",
      "Epoch 2/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.4438401460647583\n",
      "Validation MAE: 0.6720663905143738\n",
      "Epoch 3/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.4510931968688965\n",
      "Validation MAE: 0.6420608162879944\n",
      "Epoch 4/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.4670115113258362\n",
      "Validation MAE: 0.784242570400238\n",
      "Epoch 5/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.5056880712509155\n",
      "Validation MAE: 1.0457717180252075\n",
      "Epoch 6/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.5536084175109863\n",
      "Validation MAE: 0.7142295241355896\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 150\n",
    "\n",
    "# Initialize early stopping parameters\n",
    "best_val_mae = float('inf')\n",
    "patience = 5  # for early stopping\n",
    "counter = 0\n",
    "best_weights = None\n",
    "\n",
    "# Create optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# optimizer.build(model.trainable_weights)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Split data into batches\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=1024, seed=4321).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.shuffle(buffer_size=1024, seed=4321).batch(batch_size)\n",
    "\n",
    "# MAE metrics\n",
    "train_mae_metric = tf.keras.metrics.MeanAbsoluteError()\n",
    "val_mae_metric = tf.keras.metrics.MeanAbsoluteError()\n",
    "\n",
    "\n",
    "# Custom training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # Initialize tqdm with the number of steps (batches) per epoch\n",
    "    prog_bar = tqdm(dataset, unit=\"step\", leave=False)\n",
    "\n",
    "    for step, (x_batch, y_batch) in enumerate(prog_bar):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x_batch)\n",
    "\n",
    "            # Forward pass\n",
    "            y = model(x_batch, training=True)\n",
    "\n",
    "            # Calculate loss\n",
    "            losses = []\n",
    "            for output in model.layers[1].outputs:\n",
    "                loss = loss_fn(y_batch, output)\n",
    "                losses.append(loss)\n",
    "            total_loss = sum(losses)\n",
    "\n",
    "            # Log training loss and MAE to TensorBoard\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('Training Loss', total_loss, step=epoch)\n",
    "                tf.summary.scalar('Training MAE', train_mae_metric.result(), step=epoch)\n",
    "\n",
    "            # Update tqdm description with current loss\n",
    "            prog_bar.set_description(f\"Training loss: {total_loss:.4f}\")\n",
    "\n",
    "        # Update MAE metric for training data\n",
    "        train_mae_metric.update_state(y_batch, y)\n",
    "\n",
    "        # Calculate gradients\n",
    "        all_grads = []\n",
    "        for loss, scinet in zip(losses, model.layers[1].scinets):\n",
    "            grads = tape.gradient(loss, scinet.trainable_weights)\n",
    "            all_grads.append(grads)\n",
    "\n",
    "        # Update weights\n",
    "        for grads, scinet in zip(all_grads, model.layers[1].scinets):\n",
    "            optimizer.apply_gradients(zip(grads, scinet.trainable_weights))\n",
    "\n",
    "        # Clean up resources of the tape\n",
    "        del tape\n",
    "\n",
    "    # Validation loop\n",
    "    for x_val_batch, y_val_batch in val_dataset:\n",
    "        val_predictions = model(x_val_batch)\n",
    "        val_mae_metric.update_state(y_val_batch, val_predictions)\n",
    "\n",
    "        val_loss = loss_fn(y_val_batch, val_predictions)\n",
    "\n",
    "        with val_summary_writer.as_default():\n",
    "            tf.summary.scalar('Validation Loss', val_loss, step=epoch)\n",
    "            tf.summary.scalar('Validation MAE', val_mae_metric.result(), step=epoch)\n",
    "\n",
    "    # Show metrics\n",
    "    print(f\"Training MAE: {train_mae_metric.result()}\")\n",
    "    print(f\"Validation MAE: {val_mae_metric.result()}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_mae_metric.result() < best_val_mae:\n",
    "        best_val_mae = val_mae_metric.result()\n",
    "        best_weights = model.get_weights()  # save best weights\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        model.set_weights(best_weights)  # restore best weights\n",
    "        break\n",
    "\n",
    "    # Reset metrics for the next epoch\n",
    "    train_mae_metric.reset_states()\n",
    "    val_mae_metric.reset_states()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-05T21:40:40.589119Z",
     "end_time": "2023-09-05T21:46:54.479194Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7962\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Generate new id and create save directory\n",
    "# existing_ids = [int(name) for name in os.listdir('saved-models/') if name.isnumeric()]\n",
    "# run_id = random.choice(list(set(range(0, 1000)) - set(existing_ids)))\n",
    "# save_directory = f'saved-models/regressor/{run_id:03d}/'\n",
    "# os.makedirs(os.path.dirname(save_directory), exist_ok=True)\n",
    "#\n",
    "# # Save model, preprocessor and training history\n",
    "# model.save(save_directory)\n",
    "# with open(save_directory + 'preprocessor', 'wb') as f:\n",
    "#     dump(preprocessor, f, compress=3)\n",
    "# pd.DataFrame(history.history).to_csv(save_directory + 'train_history.csv')\n",
    "\n",
    "# Evaluate\n",
    "# run_id = 186\n",
    "# model = load_model(f'saved-models/regressor/{run_id:03d}/')\n",
    "# with open(f'saved-models/regressor/{run_id:03d}/preprocessor', 'rb') as f:\n",
    "#     preprocessor = load(f)\n",
    "X_test, y_test = preprocessor.transform(test_data)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)\n",
    "\n",
    "y_pred = model(X_test)\n",
    "loss = loss_fn(y_test, y_pred)\n",
    "\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "\n",
    "# # Save evaluation results\n",
    "# if not isinstance(scores, list):\n",
    "#     scores = [scores]\n",
    "# row = [run_id] + scores + [pd.Timestamp.now(tz='Australia/Melbourne')]\n",
    "# try:\n",
    "#     df_scores = pd.read_csv('saved-models/scores.csv')\n",
    "#     df_scores.loc[len(df_scores)] = row\n",
    "# except (FileNotFoundError, ValueError):\n",
    "#     df_scores = pd.DataFrame([row], columns=['id'] + list(model.metrics_names) + ['time'])\n",
    "# df_scores.to_csv('saved-models/scores.csv', index=False)\n",
    "\n",
    "# # Predict\n",
    "# # y_test is only used to calculate loss, how to get rid of it?\n",
    "# y_pred = model.predict({'inputs': X_test, 'targets': y_test})\n",
    "# y_pred = preprocessor.scaler.inverse_transform(y_pred.reshape(-1, y_test.shape[-1]))\n",
    "# y_test = preprocessor.scaler.inverse_transform(y_test.reshape(-1, y_test.shape[-1]))\n",
    "# comparison = np.hstack([y_pred, y_test])\n",
    "# df = pd.DataFrame(comparison, columns=['Predicted', 'Actual'])\n",
    "# df.to_csv(f'saved-models/regressor/{run_id:03d}/comparison.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-05T21:46:54.481588Z",
     "end_time": "2023-09-05T21:46:54.672482Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-05T21:46:54.672942Z",
     "end_time": "2023-09-05T21:46:54.674229Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
